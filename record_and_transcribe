#!/usr/bin/env python3
# record_and_transcribe.py ‚Äî GPU Whisper + Loop + DeepSeek-R1:8b + Vision (llava:7b)

from pathlib import Path
import argparse
import os
import tempfile
import subprocess
import datetime
import requests
import base64
import cv2
import re
import time


from cam_control import CameraControl
from vad_record import record_vad
from faster_whisper import WhisperModel
from gtts import gTTS

# ---------- Paths & Models ----------
PIPER_BIN   = os.environ.get("PIPER_BIN", str(Path.home() / "Desktop/piper/piper"))
PIPER_MODEL = os.environ.get("PIPER_MODEL", str(Path.home() / "Desktop/piper_voices/en_US-ryan-medium.onnx"))

# Text LLM (DeepSeek-R1) ‚Äî OpenAI compatible endpoint
OLLAMA_URL        = os.environ.get("OLLAMA_URL", "http://127.0.0.1:11434/v1/chat/completions")
OLLAMA_MODEL      = os.environ.get("LLM_MODEL", "deepseek-r1:8b")   # Text model

# Vision LLM (LLaVA) ‚Äî native /api/generate endpoint
OLLAMA_VLM_MODEL  = os.environ.get("OLLAMA_VLM_MODEL", "llava:7b")
OLLAMA_NATIVE_URL = os.environ.get("OLLAMA_NATIVE_URL", "http://127.0.0.1:11434")

AUTO_DESCRIBE_ON_OPEN = False



# ---------- Voice command phrase lists (global) ----------
OPEN_CAM_PHRASES = [
    "open cam", "open camera", "turn on camera","turn on the camera",
    "kamera a√ß", "kamera ac", "cam a√ß", "cam ac"
]

CLOSE_CAM_PHRASES = [
    "close cam", "close camera", "turn off camera","turn off the camera",
    "kamera baƒüla", "kamera bagla", "cam baƒüla", "cam bagla"
]

SEE_PHRASES = [
    "what do you see", "what do you see in front", "see in front of the camera",
    "what is in front of the camera", "what do you see right now",
    "describe what you see", "n…ô g√∂r√ºrs…ôn", "ne gorursen"
]

SEE_REGEX = re.compile(
    r"(what\s+do\s+you\s+see(\s+in\s+front(\s+of\s+the\s+camera)?)?|"
    r"what\s+is\s+in\s+front\s+of\s+the\s+camera|"
    r"describe\s+(this|the\s+scene)|"
    r".*see.*camera)",
    re.IGNORECASE
)
IMAGE_QUESTION_PHRASES = [
    "what is in the picture",
    "what is in this picture",
    "what is in the image",
    "what is in this image",
    "what is in the photo",
    "what is in this photo",
    "describe the picture",
    "describe this picture",
    "describe the image",
    "describe this image",
    "describe the photo",
    "describe this photo",
]

IMAGE_QUESTION_REGEX = re.compile(
    r"(what\s+is\s+in\s+(this\s+)?(picture|image|photo)|"
    r"describe\s+(this\s+)?(picture|image|photo))",
    re.IGNORECASE
)


WHAT_IS_THIS_PHRASES = [
    "what is this", "what is this?", "bu nedir", "bu n…ôdir"
]

SHOW_CAM_PHRASES = [
    "show camera", "show cam", "open preview", "kamera g√∂st…ôr", "kamera goster", "p…ônc…ôr…ôni a√ß", "pencereni ac"
]

HIDE_CAM_PHRASES = [
    "hide camera", "hide cam", "close preview", "p…ônc…ôr…ôni baƒüla", "pencereni bagla"
]

EMOTION_PHRASES = [
    "am i smiling", "am i angry", "am i happy", "am i sad",
    "what is my expression", "do i look happy", "do i look angry",
    "√ºz ifad…ôm n…ôdir", "men indi esebiyem", "m…ôn indi …ôs…ôbiy…ôm", "g√ºl√ºrem", "gulurem"
]
WHAT_IS_THIS_PHRASES = [
    "what is this", "what is this?", "bu nedir", "bu n…ôdir"
]

HOLDING_ITEM_PHRASES = [
    "what am i holding",
    "what's the item i'm holding",
    "what is the item i'm holding",
    "what is in my hand",
    "what's in my hand",
    "what item am i holding",
]

HOLDING_ITEM_REGEX = re.compile(
    r"(what('| i)?s\s+the\s+item\s+i('m| am)\s+holding|"
    r"what\s+am\s+i\s+holding|"
    r"what('| i)?s\s+in\s+my\s+hand)",
    re.IGNORECASE,
)



# ---------- TTS ----------
def speak_gtts(text: str, lang="en"):
    """Online fallback TTS with gTTS (+ mpg123)."""
    try:
        tmp_mp3 = "/tmp/tts.mp3"
        gTTS(text=text, lang=lang).save(tmp_mp3)
        subprocess.run(["mpg123", "-q", tmp_mp3], check=False)
    except Exception as e:
        print("TTS(gTTS) error:", e)


def piper_tts(text: str):
    """Offline TTS using Piper ‚Üí plays with 'aplay' (simple and robust)."""
    if not Path(PIPER_BIN).exists():
        raise RuntimeError(f"Piper binary not found: {PIPER_BIN}")
    if not Path(PIPER_MODEL).exists():
        raise RuntimeError(f"Piper model not found:  {PIPER_MODEL}")

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tf:
        outwav = tf.name

    cmd = [
        PIPER_BIN,
        "--model", PIPER_MODEL,
        "--output_file", outwav,
        "--length_scale", "1.30",
        "--noise_scale", "0.33",
        "--noise_w", "0.55",
        "--sentence_silence", "0.20",
    ]
    p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    _, err = p.communicate(input=text.encode("utf-8"))
    if p.returncode != 0:
        raise RuntimeError(f"Piper failed: {err.decode(errors='ignore')}")
    try:
        subprocess.run(["aplay", "-q", outwav], check=False)
    finally:
        try:
            os.unlink(outwav)
        except Exception:
            pass



        

# ---------- Vision LLM helper ----------
# ---------- Vision LLM helper ----------
def vlm_describe_b64(image_b64: str, prompt="Describe the scene briefly."):
    """
    Send a base64-encoded JPEG frame to Ollama vision model (llava:7b)
    using the native /api/generate endpoint with `images` array.
    """
    payload = {
        "model": OLLAMA_VLM_MODEL,
        "prompt": prompt,
        "stream": False,
        # IMPORTANT: here we pass *raw* base64, NOT "data:image/jpeg;base64,..."
        "images": [image_b64],
    }

    try:
        # üî¥ BURANI D∆èYƒ∞≈û
        r = requests.post(f"{OLLAMA_NATIVE_URL}/api/generate", json=payload, timeout=60)
        r.raise_for_status()
        data = r.json()
        return (data.get("response") or "").strip() or "[VLM: empty response]"
    except Exception as e:
        return f"[VLM error: {e}]"











# ---------- Text LLM ----------
def ollama_chat(user_text: str) -> str:
    """Minimal chat-completions call to local Ollama."""
    now = datetime.datetime.now().strftime("%B %d, %Y, %H:%M")
    system_prompt = (
        f"You are an offline assistant. The current real date and time is {now}. "
        "Respond naturally and briefly. Never claim you don't know today's date."
    )
    payload = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_text}
        ],
        "stream": False
    }
    try:
        r = requests.post(OLLAMA_URL, json=payload, timeout=60)
        r.raise_for_status()
        data = r.json()
        return data["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print("‚ùå LLM request failed:", e)
        return "Error: LLM request failed."








# ---------- One turn (VAD ‚Üí Whisper ‚Üí LLM ‚Üí TTS) ----------
def handle_single_turn(model: WhisperModel, tts_engine: str, no_tts: bool, cam: CameraControl) -> bool:


    def grab_frame(cam, retries=20, delay=0.05):
        for _ in range(retries):
            f = cam.get_frame()
            if f is not None:
                return f
            time.sleep(delay)
        return None


    
    # ---- normalize helpers ----
    def norm_text(t: str) -> str:
        return " ".join(t.lower().strip().split())

    def has_any(text: str, phrases) -> bool:
        return any(p in text for p in phrases)

    out = Path.home() / "Desktop" / "output.wav"
    file_path = record_vad(output=str(out), silence_limit=1.2, device=None)
    if not file_path or not Path(file_path).exists():
        print("‚ùå S…ôs yazƒ±la bilm…ôdi.")
        return True

    print(f"üéß Yazƒ±lmƒ±≈ü fayl: {file_path}")
    print("üß† Whisper transkripsiya ba≈ülayƒ±r...")

    segments, info = model.transcribe(str(file_path), language="en", task = "transcribe", vad_filter=True, beam_size=1, best_of=1)
    print(f"üåê Dil: {info.language}")
    text = "".join(s.text for s in segments).strip()
    print("\nüìù N…ôtic…ô:")
    print(text or "[bo≈ü]")

    if not text:
        return True

    low = norm_text(text)

    if low.startswith("watch the item i am holding"):
        low = "what is the item i am holding"
    elif low.startswith("watch the item i'm holding"):
        low = "what is the item i'm holding"

    
    # exit
    if low in {"exit", "quit", "stop"}:
        print("üëã √áƒ±xƒ±≈ü …ômri verildi. Saƒü ol!")
        return False

    # --- Camera voice commands (robust) ---
    if has_any(low, OPEN_CAM_PHRASES):
        if not cam.open():
            out_msg = "Camera failed to open."
        else:
            out_msg = "Camera opened."
        print("üì∑", out_msg)
        if not no_tts and tts_engine != "off":
            (piper_tts if tts_engine == "piper" else speak_gtts)(out_msg)
        return True
    
    
    if has_any(low, EMOTION_PHRASES):
        frame = grab_frame(cam)
        if frame is None:
            msg = "Camera is open but no frame yet."
        else:
            ok, buf = cv2.imencode(".jpg", frame, [int(cv2.IMWRITE_JPEG_QUALITY), 88])
            if not ok:
                msg = "Failed to encode frame."
            else:
                b64 = base64.b64encode(buf.tobytes()).decode("ascii")
                # Sad…ô, etibarlƒ± prompt: qƒ±sa cavab + inam d…ôr…ôc…ôsi
                prompt = (
                    "Analyze the person's facial expression in this image. "
                    "Answer briefly with the most likely emotion (e.g., neutral, happy/smiling, angry, sad, surprised) "
                    "and provide a confidence level 0-100%. If the face is not clearly visible, say 'uncertain'."
                )
                msg = vlm_describe_b64(b64, prompt=prompt)
        print("üôÇ Emotion:", msg)
        if not no_tts and tts_engine != "off":
            (piper_tts if tts_engine=="piper" else speak_gtts)(msg)
        return True

    
    
    if has_any(low, CLOSE_CAM_PHRASES):
        cam.close()
        out_msg = "Camera closed."
        print("üì∑", out_msg)
        if not no_tts and tts_engine != "off":
            (piper_tts if tts_engine == "piper" else speak_gtts)(out_msg)
        return True

    if has_any(low, SEE_PHRASES) or SEE_REGEX.search(low):
        frame = cam.get_frame()
        if frame is None:
            msg = "Camera is not open or no frame yet."
        else:
            ok, buf = cv2.imencode(".jpg", frame, [int(cv2.IMWRITE_JPEG_QUALITY), 85])
            if not ok:
                msg = "Failed to encode frame."
            else:
                b64 = base64.b64encode(buf.tobytes()).decode("ascii")
                msg = vlm_describe_b64(
                    b64,
                    prompt="Describe briefly (1‚Äì2 sentences). If any text exists, include OCR at the end."
                )
        print("üñºÔ∏è Vision:", msg)
        if not no_tts and tts_engine != "off":
            (piper_tts if tts_engine == "piper" else speak_gtts)(msg)
        return True


    if has_any(low, SHOW_CAM_PHRASES):
        cam.show_preview()
        out_msg = "Camera preview opened."
        print("ü™ü", out_msg)
        if not no_tts and tts_engine != "off":
            (piper_tts if tts_engine=="piper" else speak_gtts)(out_msg)
        return True

    if has_any(low, HIDE_CAM_PHRASES):
        cam.hide_preview()
        out_msg = "Camera preview closed."
        print("ü™ü", out_msg)
        if not no_tts and tts_engine != "off":
            (piper_tts if tts_engine=="piper" else speak_gtts)(out_msg)
        return True




    
    if (
        has_any(low, WHAT_IS_THIS_PHRASES)
        or has_any(low, HOLDING_ITEM_PHRASES)
        or HOLDING_ITEM_REGEX.search(low)
    ):

        frame = cam.get_frame()
        if frame is None:
            msg = "Camera is not open or no frame yet."
        else:
            ok, buf = cv2.imencode(".jpg", frame, [int(cv2.IMWRITE_JPEG_QUALITY), 88])
            if not ok:
                msg = "Failed to encode frame."
            else:
                b64 = base64.b64encode(buf.tobytes()).decode("ascii")
                prompt = (
                    "Identify the single most likely object the person is holding in this image. "
                    "Give exactly ONE answer in the format '<name> ‚Äî short reason'. "
                    "If any text or logos are visible, mention them in the reason. "
                    "Do NOT list multiple options."
                )

                msg = vlm_describe_b64(b64, prompt=prompt)
        print("üñºÔ∏è Vision:", msg)
        if not no_tts and tts_engine != "off":
            (piper_tts if tts_engine == "piper" else speak_gtts)(msg)
        return True

    if low.startswith("analyze frame:"):
        q = text.split(":", 1)[1].strip() or "Describe this image."
        frame = cam.get_frame()
        if frame is None:
            msg = "Camera is not open or no frame yet."
        else:
            ok, buf = cv2.imencode(".jpg", frame, [int(cv2.IMWRITE_JPEG_QUALITY), 90])
            if ok:
                b64 = base64.b64encode(buf.tobytes()).decode("ascii")
                msg = vlm_describe_b64(b64, prompt=q)
            else:
                msg = "Failed to encode frame."
        print("üñºÔ∏è Vision:", msg)
        if not no_tts and tts_engine != "off":
            (piper_tts if tts_engine == "piper" else speak_gtts)(msg)
        return True
    # --- END camera commands ---

    # Fallback: plain LLM reply
    reply = ollama_chat(text)
    print("\nü§ñ Reply:\n" + reply + "\n")

    if not no_tts and tts_engine != "off":
        try:
            if tts_engine == "piper":
                piper_tts(reply)
            else:
                speak_gtts(reply)
        except Exception as e:
            print("TTS error:", e)

    return True

# ---------- Loop mode ----------
def loop_mode(tts_engine: str, no_tts: bool):
    print("üé§ Loop mode ...")
    try:
        print("üß† Loading Whisper on GPU (cuda, int8_float16)...")
        model = WhisperModel("small.en", device="cuda", compute_type="int8_float16")

    except Exception as e:
        print(f"‚ö†Ô∏è CUDA init failed: {e}\n‚û°Ô∏è Falling back to CPU (int8).")
        model = WhisperModel("small.en", device="cuda", compute_type="int8_float16")


    cam = CameraControl(index=0)   # default webcam

    try:
        while True:
            cont = handle_single_turn(model, tts_engine, no_tts, cam)
            if not cont:
                break
    except KeyboardInterrupt:
        print("\nüëã Ctrl+C ‚Äî √ßƒ±xƒ±lƒ±r.")
    finally:
        cam.close()

# ---------- CLI ----------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--text", type=str, help="If provided, skip mic/VAD and chat with this text (single run).")
    ap.add_argument("--no-tts", action="store_true", help="Disable TTS playback.")
    ap.add_argument("--tts", choices=["piper", "gtts", "off"], default="piper",
                    help="Choose TTS engine (default: piper)")
    ap.add_argument("--once", action="store_true", help="Run only one turn instead of loop.")
    args = ap.parse_args()

    if args.text is not None:
        user_text = args.text.strip()
        if not user_text:
            print("Empty --text input."); return
        print(f"üìù Text input: {user_text}")
        reply = ollama_chat(user_text)
        print("\nü§ñ Reply:\n" + reply + "\n")
        if not args.no_tts and args.tts != "off":
            try:
                piper_tts(reply) if args.tts == "piper" else speak_gtts(reply)
            except Exception as e:
                print("TTS error:", e)
        return

    if args.once:
        try:
            print("üß† Loading Whisper on GPU (cuda, int8_float16)...")
            mmodel = WhisperModel("small.en", device="cuda", compute_type="int8_float16")

        except Exception as e:
            print(f"‚ö†Ô∏è CUDA init failed: {e}\n‚û°Ô∏è Falling back to CPU (int8).")
            model = WhisperModel("small.en", device="cuda", compute_type="int8_float16")

        cam = CameraControl(index=0)
        try:
            _ = handle_single_turn(model, args.tts, args.no_tts, cam)
        finally:
            cam.close()
    else:
        loop_mode(args.tts, args.no_tts)

if __name__ == "__main__":
    main()
